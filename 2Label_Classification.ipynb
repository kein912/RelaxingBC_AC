{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SVM\n",
    "from sklearn import svm\n",
    "from sklearn import datasets\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import train_test_split\n",
    "# KNN\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "# LR\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "# DT\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "# RF\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import precision_score, recall_score, roc_auc_score, confusion_matrix\n",
    "# AB\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "# GB\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "# XGB\n",
    "from xgboost import XGBClassifier\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#LGB\n",
    "from lightgbm import LGBMClassifier\n",
    "#DNN\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "#改動版(k-fold)\n",
    "\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.model_selection import LeaveOneOut\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score\n",
    "import numpy as np\n",
    "\n",
    "def Classifier333(name,data, label, result, datatype):\n",
    "\n",
    "    #model_name = [\"SVM\",\"KNN\",\"LR\",\"DT\",\"RF\",\"AdaBoost\",\"GradientBoost\",\"XGBoost\",\"LGBM\",\"DNN\"]\n",
    "    model_name = [\"SVM\",\"KNN\",\"LR\",\"DT\",\"RF\",\"XGBoost\"]\n",
    "    model_params =  {\n",
    "        SVC(probability=True, random_state=42),\n",
    "        KNeighborsClassifier(n_neighbors=3, weights='uniform', algorithm='auto'),\n",
    "        LogisticRegression(random_state=42, max_iter=1000),\n",
    "        DecisionTreeClassifier(random_state=42),\n",
    "        RandomForestClassifier(n_estimators=100),        \n",
    "        XGBClassifier(random_state=42, objective='binary:logistic', nthread=-1)       \n",
    "    }\n",
    "    '''\n",
    "    AdaBoostClassifier(random_state=42),\n",
    "    GradientBoostingClassifier(random_state=42),\n",
    "    LGBMClassifier(random_state=42, objective='binary', n_jobs=-1)\n",
    "    '''\n",
    "\n",
    "    # 创建SVM分类器\n",
    "    svm = SVC()\n",
    "\n",
    "    Accuracy = []\n",
    "    Recall = []\n",
    "    Precision = []\n",
    "    F1 = []\n",
    "    Specificity = []\n",
    "    AUC = []\n",
    "    for i, model in enumerate(model_params):\n",
    "        # 创建留一法交叉验证\n",
    "        loo = LeaveOneOut()\n",
    "\n",
    "         # 存储预测结果和真实标签\n",
    "        predictions = []\n",
    "        true_labels = []\n",
    "        # print('\\n',model_name[i],'\\n')\n",
    "\n",
    "        clf = model\n",
    "        for train_index, test_index in loo.split(data):\n",
    "            X_train, X_test = data.iloc[train_index], data.iloc[test_index]\n",
    "            y_train, y_test = [label[i] for i in train_index], [label[i] for i in test_index]\n",
    "\n",
    "            # 训练模型\n",
    "            clf.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "            y_pred = clf.predict(X_test)\n",
    "            # 保存结果\n",
    "            predictions.extend(y_pred)\n",
    "            true_labels.extend(y_test)\n",
    "        # 计算准确度\n",
    "        accuracy = accuracy_score(true_labels, predictions)\n",
    "        accuracy = round(accuracy,2)\n",
    "        recall = recall_score(true_labels, predictions)\n",
    "        precision = precision_score(true_labels, predictions)\n",
    "        f1 = f1_score(true_labels, predictions)\n",
    "        Accuracy.append(accuracy)\n",
    "        Recall.append(recall)\n",
    "        Precision.append(precision)\n",
    "        F1.append(f1)\n",
    "        tn, fp, fn, tp = confusion_matrix(true_labels, predictions).ravel()\n",
    "        specificity = tn / (tn + fp)\n",
    "        Specificity.append(specificity)\n",
    "\n",
    "        \n",
    "        \n",
    "        \n",
    "    '''\n",
    "    # 创建留一法交叉验证\n",
    "    loo = LeaveOneOut()\n",
    "\n",
    "    # 存储预测结果和真实标签\n",
    "    predictions = []\n",
    "    true_labels = []\n",
    "    # 对每个留一组合进行训练和测试\n",
    "    for train_index, test_index in loo.split(data):\n",
    "        X_train, X_test = data.iloc[train_index], data.iloc[test_index]\n",
    "        y_train, y_test = [label[i] for i in train_index], [label[i] for i in test_index]\n",
    "\n",
    "        # 训练模型\n",
    "        svm.fit(X_train, y_train)\n",
    "\n",
    "        # 预测\n",
    "        y_pred = svm.predict(X_test)\n",
    "\n",
    "        # 保存结果\n",
    "        predictions.extend(y_pred)\n",
    "        true_labels.extend(y_test)\n",
    "    '''\n",
    "    \n",
    "   \n",
    "\n",
    "    Classifier = pd.DataFrame(model_name, columns=['Classifier'])\n",
    "    Accuracy = pd.DataFrame(Accuracy, columns=['Accuracy']) # 模型正確預測的樣本數占總數樣本數的比例 (TP+TN)/(TP+TN+FP+FN)\n",
    "    Recall = pd.DataFrame(Recall, columns=['Recall']) # Sensitivity, 實際為正且被模型預測為正的樣本數佔所有實際為正的樣本數比例 TP/(TP+FN)\n",
    "    Specificity = pd.DataFrame(Specificity, columns=['Specificity']) # 實際為負且被模型正確預測為負的樣本數占所有實際為負的樣本數的比例 TN/(TN+FP)\n",
    "    Precision = pd.DataFrame(Precision, columns=['Precision']) # 實際為正且被模型預測為正的樣本數佔模型預測為正的樣本數比例 TP/(TP+FP)\n",
    "    F1 = pd.DataFrame(F1, columns=['F1']) # Precision 和 Recall 的調和平均數。F1 分數在 Precision 和 Recall 之間取平衡，特別適合類別不平衡的情況 2*(Precision*Recall)/(Precision+Recall)\n",
    "    AUC = pd.DataFrame(AUC, columns=['AUC']) # 被用作評價模型對樣本的排序能力，即正樣本的預測概率是否高於負樣本的預測概率。\n",
    "    result = pd.concat([result, Classifier, Accuracy, Recall, Specificity, Precision, F1, AUC], axis=1)\n",
    "    result.to_excel(f'{name}_{datatype}_333classfication.xlsx', index=None)\n",
    "    print(result)\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "\n",
    "def Classifier(name,data, label, result, datatype):\n",
    " \n",
    "    model_name = [\"SVM\",\"KNN\",\"LR\",\"DT\",\"RF\",\"AdaBoost\",\"GradientBoost\",\"XGBoost\",\"LGBM\",\"DNN\"]\n",
    "    model_params =  {\n",
    "        SVC(probability=True, random_state=42),\n",
    "        KNeighborsClassifier(n_neighbors=3, weights='uniform', algorithm='auto'),\n",
    "        LogisticRegression(random_state=42, max_iter=1000),\n",
    "        DecisionTreeClassifier(random_state=42),\n",
    "        RandomForestClassifier(n_estimators=100),\n",
    "        AdaBoostClassifier(random_state=42),\n",
    "        GradientBoostingClassifier(random_state=42),\n",
    "        XGBClassifier(random_state=42, objective='binary:logistic', nthread=-1),\n",
    "        LGBMClassifier(random_state=42, objective='binary', n_jobs=-1)\n",
    "    }\n",
    "    \n",
    "    X = data\n",
    "    y = label\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "    print(len(X_train),len(y_train),len(X_test),len(y_test))\n",
    "    Accuracy = []\n",
    "    Recall = []\n",
    "    Precision = []\n",
    "    F1 = []\n",
    "    Specificity = []\n",
    "    AUC = []\n",
    "    for i, model in enumerate(model_params):\n",
    "        # print('\\n',model_name[i],'\\n')\n",
    "\n",
    "        clf = model\n",
    "        clf.fit(X_train, y_train)\n",
    "\n",
    "        #　# 預測\n",
    "        y_pred = clf.predict(X_test)\n",
    "        y_pred_proba = clf.predict_proba(X_test)[:,1]  # 如果是二分類的情況\n",
    "        # print(y_test)\n",
    "        # print(y_pred)\n",
    "\n",
    "        # 評估模型效果\n",
    "        accuracy = accuracy_score(y_test, y_pred)\n",
    "        accuracy = round(accuracy,2)\n",
    "        recall = recall_score(y_test, y_pred)\n",
    "        precision = precision_score(y_test, y_pred)\n",
    "        f1 = f1_score(y_test, y_pred)\n",
    "        Accuracy.append(accuracy)\n",
    "        Recall.append(recall)\n",
    "        Precision.append(precision)\n",
    "        F1.append(f1)\n",
    "\n",
    "        # 计算 Specificity\n",
    "        # TN: 模型預測為負例，且實際上也是負例的樣本數。\n",
    "        # FP: 模型預測為正例，但實際上是負例的樣本數。\n",
    "        # FN: 模型預測為負例，但實際上是正例的樣本數。\n",
    "        # TP: 模型預測為正例，且實際上也是正例的樣本數。\n",
    "        tn, fp, fn, tp = confusion_matrix(y_test, y_pred).ravel()\n",
    "        specificity = tn / (tn + fp)\n",
    "        Specificity.append(specificity)\n",
    "        # if specificity is not None and specificity != 0.0:\n",
    "            # print(y_test)\n",
    "            # print(y_pred)\n",
    "\n",
    "        # 如果是二分類的情況，可以計算AUC\n",
    "        auc = roc_auc_score(y_test, y_pred_proba)\n",
    "        AUC.append(auc)\n",
    "\n",
    "        # print(f\"Accuracy: {accuracy * 100:.2f}%\")\n",
    "        # print(f\"Recall: {recall * 100:.2f}%\")\n",
    "        # print(f\"Precision: {precision * 100:.2f}%\")\n",
    "        # print(f\"AUC: {auc:.2f}\")\n",
    "\n",
    "    Classifier = pd.DataFrame(model_name, columns=['Classifier'])\n",
    "    Accuracy = pd.DataFrame(Accuracy, columns=['Accuracy']) # 模型正確預測的樣本數占總數樣本數的比例 (TP+TN)/(TP+TN+FP+FN)\n",
    "    Recall = pd.DataFrame(Recall, columns=['Recall']) # Sensitivity, 實際為正且被模型預測為正的樣本數佔所有實際為正的樣本數比例 TP/(TP+FN)\n",
    "    Specificity = pd.DataFrame(Specificity, columns=['Specificity']) # 實際為負且被模型正確預測為負的樣本數占所有實際為負的樣本數的比例 TN/(TN+FP)\n",
    "    Precision = pd.DataFrame(Precision, columns=['Precision']) # 實際為正且被模型預測為正的樣本數佔模型預測為正的樣本數比例 TP/(TP+FP)\n",
    "    F1 = pd.DataFrame(F1, columns=['F1']) # Precision 和 Recall 的調和平均數。F1 分數在 Precision 和 Recall 之間取平衡，特別適合類別不平衡的情況 2*(Precision*Recall)/(Precision+Recall)\n",
    "    AUC = pd.DataFrame(AUC, columns=['AUC']) # 被用作評價模型對樣本的排序能力，即正樣本的預測概率是否高於負樣本的預測概率。\n",
    "    result = pd.concat([result, Classifier, Accuracy, Recall, Specificity, Precision, F1, AUC], axis=1)\n",
    "    result.to_excel(f'{name}_{datatype}_classfication.xlsx', index=None)\n",
    "    print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def DNN_Model(name,data, label, result,datatype, featuresCount):\n",
    "\n",
    "    # 创建深度神经网络模型\n",
    "    dnn_model = keras.Sequential([\n",
    "        keras.layers.Dense(64, activation='relu', input_shape=(featuresCount,)),  # 输入层\n",
    "        keras.layers.Dense(128, activation='relu'),  # 隐藏层1\n",
    "        keras.layers.Dropout(0.5),  # 添加Dropout层以减少过拟合\n",
    "        keras.layers.Dense(64, activation='relu'),  # 隐藏层2\n",
    "        keras.layers.Dropout(0.5),\n",
    "        keras.layers.Dense(1, activation='sigmoid')  # 输出层（二分类问题，使用Sigmoid激活）\n",
    "    ])\n",
    "\n",
    "    # 编译模型\n",
    "    dnn_model.compile(optimizer='adam',\n",
    "                loss='binary_crossentropy',  # 二分类交叉熵损失函数\n",
    "                metrics=['accuracy', keras.metrics.Precision(), keras.metrics.Recall()])\n",
    "    \n",
    "\n",
    "    model_name = [\"DNN\"]\n",
    "    model_params =  {\n",
    "        dnn_model\n",
    "    }\n",
    "    \n",
    "    X = data\n",
    "    X.shape\n",
    "    y = label\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "    X_train = np.array(X_train)\n",
    "    X_test = np.array(X_test)\n",
    "    y_train = np.array(y_train)\n",
    "    y_test = np.array(y_test)\n",
    "\n",
    "    print(len(X_train),len(y_train),len(X_test),len(y_test))\n",
    "    # print(y_train,y_test)\n",
    "    Accuracy = []\n",
    "    Recall = []\n",
    "    Precision = []\n",
    "    F1 = []\n",
    "    Specificity = []\n",
    "    AUC = []\n",
    "    for i, model in enumerate(model_params):\n",
    "        # print('\\n',model_name[i],'\\n')\n",
    "        \n",
    "        # 训练模型\n",
    "        clf= model\n",
    "        clf.fit(X_train, y_train, epochs=10, batch_size=16, validation_split=0.2, verbose=2)\n",
    "\n",
    "        #　# 預測\n",
    "        y_pred = clf.predict(X_test)\n",
    "        # y_pred_proba = clf.predict_proba(X_test)[:,1]  # 如果是二分類的情況\n",
    "        print(y_test)\n",
    "\n",
    "        # 使用适当的阈值进行二进制分类\n",
    "        threshold = 0.5\n",
    "        y_pred_binary = (y_pred > threshold).astype(int)\n",
    "        # print(y_pred_binary)\n",
    "        \n",
    "        # 计算准确率、召回率、精确率和F1分数\n",
    "        accuracy = accuracy_score(y_test, y_pred_binary)\n",
    "        accuracy = round(accuracy,2)\n",
    "        recall = recall_score(y_test, y_pred_binary)\n",
    "        precision = precision_score(y_test, y_pred_binary)\n",
    "        f1 = f1_score(y_test, y_pred_binary, average='macro')\n",
    "\n",
    "        Accuracy.append(accuracy)\n",
    "        Recall.append(recall)\n",
    "        Precision.append(precision)\n",
    "        F1.append(f1)\n",
    "\n",
    "        # 计算 Specificity\n",
    "        # TN: 模型預測為負例，且實際上也是負例的樣本數。\n",
    "        # FP: 模型預測為正例，但實際上是負例的樣本數。\n",
    "        # FN: 模型預測為負例，但實際上是正例的樣本數。\n",
    "        # TP: 模型預測為正例，且實際上也是正例的樣本數。\n",
    "        tn, fp, fn, tp = confusion_matrix(y_test, y_pred_binary).ravel()\n",
    "        specificity = tn / (tn + fp)\n",
    "        Specificity.append(specificity)\n",
    "        # if specificity is not None and specificity != 0.0:\n",
    "            # print(y_test)\n",
    "            # print(y_pred)\n",
    "\n",
    "        # 如果是二分類的情況，可以計算AUC\n",
    "        # auc = roc_auc_score(y_test, y_pred_proba)\n",
    "        # AUC.append(auc)\n",
    "\n",
    "        # print(f\"Accuracy: {accuracy * 100:.2f}%\")\n",
    "        # print(f\"Recall: {recall * 100:.2f}%\")\n",
    "        # print(f\"Precision: {precision * 100:.2f}%\")\n",
    "        # print(f\"AUC: {auc:.2f}\")\n",
    "\n",
    "    Classifier = pd.DataFrame(model_name, columns=['Classifier'])\n",
    "    Accuracy = pd.DataFrame(Accuracy, columns=['Accuracy']) # 模型正確預測的樣本數占總數樣本數的比例 (TP+TN)/(TP+TN+FP+FN)\n",
    "    Recall = pd.DataFrame(Recall, columns=['Recall']) # Sensitivity, 實際為正且被模型預測為正的樣本數佔所有實際為正的樣本數比例 TP/(TP+FN)\n",
    "    Specificity = pd.DataFrame(Specificity, columns=['Specificity']) # 實際為負且被模型正確預測為負的樣本數占所有實際為負的樣本數的比例 TN/(TN+FP)\n",
    "    Precision = pd.DataFrame(Precision, columns=['Precision']) # 實際為正且被模型預測為正的樣本數佔模型預測為正的樣本數比例 TP/(TP+FP)\n",
    "    F1 = pd.DataFrame(F1, columns=['F1']) # Precision 和 Recall 的調和平均數。F1 分數在 Precision 和 Recall 之間取平衡，特別適合類別不平衡的情況 2*(Precision*Recall)/(Precision+Recall)\n",
    "    AUC = pd.DataFrame(AUC, columns=['AUC']) # 被用作評價模型對樣本的排序能力，即正樣本的預測概率是否高於負樣本的預測概率。\n",
    "    result = pd.concat([result, Classifier, Accuracy, Recall, Specificity, Precision, F1, AUC], axis=1)\n",
    "    result.to_excel(f'{name}_{datatype}_DNN_classfication.xlsx', index=None)\n",
    "    print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gameTime_df = pd.read_excel(\"gameTime_W.xlsx\")\n",
    "gameTime_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tasks = ['C', 'A', 'W']\n",
    "Tasks = ['W']\n",
    "# Tasks = ['CAW', 'CA', 'CW', 'AW']\n",
    "# DataTypes = ['EM', 'HM', 'TP', 'EEG']\n",
    "# DataTypes = ['EH', 'ET', 'EG', 'HT', 'HG', 'TG']\n",
    "# DataTypes = ['EHT', 'EHG', 'ETG', 'HTG', 'EHTG']\n",
    "#DataTypes = ['HM+TP','EM+HM+TP']\n",
    "#DataTypes = ['EM+HM','HM+TP','EM+HM+TP','EM+HM+TP+EEG']\n",
    "DataTypes = ['EM+HM']\n",
    "# DataTypes = ['HT', 'EHT', 'EHTFp1', 'EHTG']\n",
    "#DataTypes = ['HTAF8', 'EHTAF8']\n",
    "\n",
    "#Stage = ['Balloon', 'Cake']\n",
    "Stage = ['Balloon']\n",
    "InfOrNot = ['Inf', 'NoInf', 'All']\n",
    "TaskFull = {\"C\":\"Cpt\", \"A\":\"Audio\", \"W\":\"Wcst\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\wmlab\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\wmlab\\anaconda3\\lib\\site-packages\\sklearn\\neighbors\\_classification.py:228: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "c:\\Users\\wmlab\\anaconda3\\lib\\site-packages\\sklearn\\neighbors\\_classification.py:228: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "c:\\Users\\wmlab\\anaconda3\\lib\\site-packages\\sklearn\\neighbors\\_classification.py:228: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "c:\\Users\\wmlab\\anaconda3\\lib\\site-packages\\sklearn\\neighbors\\_classification.py:228: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "c:\\Users\\wmlab\\anaconda3\\lib\\site-packages\\sklearn\\neighbors\\_classification.py:228: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "c:\\Users\\wmlab\\anaconda3\\lib\\site-packages\\sklearn\\neighbors\\_classification.py:228: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "c:\\Users\\wmlab\\anaconda3\\lib\\site-packages\\sklearn\\neighbors\\_classification.py:228: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "c:\\Users\\wmlab\\anaconda3\\lib\\site-packages\\sklearn\\neighbors\\_classification.py:228: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "c:\\Users\\wmlab\\anaconda3\\lib\\site-packages\\sklearn\\neighbors\\_classification.py:228: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "c:\\Users\\wmlab\\anaconda3\\lib\\site-packages\\sklearn\\neighbors\\_classification.py:228: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "c:\\Users\\wmlab\\anaconda3\\lib\\site-packages\\sklearn\\neighbors\\_classification.py:228: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "c:\\Users\\wmlab\\anaconda3\\lib\\site-packages\\sklearn\\neighbors\\_classification.py:228: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "c:\\Users\\wmlab\\anaconda3\\lib\\site-packages\\sklearn\\neighbors\\_classification.py:228: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "c:\\Users\\wmlab\\anaconda3\\lib\\site-packages\\sklearn\\neighbors\\_classification.py:228: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "c:\\Users\\wmlab\\anaconda3\\lib\\site-packages\\sklearn\\neighbors\\_classification.py:228: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "c:\\Users\\wmlab\\anaconda3\\lib\\site-packages\\sklearn\\neighbors\\_classification.py:228: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "c:\\Users\\wmlab\\anaconda3\\lib\\site-packages\\sklearn\\neighbors\\_classification.py:228: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "c:\\Users\\wmlab\\anaconda3\\lib\\site-packages\\sklearn\\neighbors\\_classification.py:228: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "c:\\Users\\wmlab\\anaconda3\\lib\\site-packages\\sklearn\\neighbors\\_classification.py:228: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "c:\\Users\\wmlab\\anaconda3\\lib\\site-packages\\sklearn\\neighbors\\_classification.py:228: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "c:\\Users\\wmlab\\anaconda3\\lib\\site-packages\\sklearn\\neighbors\\_classification.py:228: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "c:\\Users\\wmlab\\anaconda3\\lib\\site-packages\\sklearn\\neighbors\\_classification.py:228: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "c:\\Users\\wmlab\\anaconda3\\lib\\site-packages\\sklearn\\neighbors\\_classification.py:228: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "c:\\Users\\wmlab\\anaconda3\\lib\\site-packages\\sklearn\\neighbors\\_classification.py:228: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "c:\\Users\\wmlab\\anaconda3\\lib\\site-packages\\sklearn\\neighbors\\_classification.py:228: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "c:\\Users\\wmlab\\anaconda3\\lib\\site-packages\\sklearn\\neighbors\\_classification.py:228: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "c:\\Users\\wmlab\\anaconda3\\lib\\site-packages\\sklearn\\neighbors\\_classification.py:228: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "c:\\Users\\wmlab\\anaconda3\\lib\\site-packages\\sklearn\\neighbors\\_classification.py:228: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "c:\\Users\\wmlab\\anaconda3\\lib\\site-packages\\sklearn\\neighbors\\_classification.py:228: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "c:\\Users\\wmlab\\anaconda3\\lib\\site-packages\\sklearn\\neighbors\\_classification.py:228: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "c:\\Users\\wmlab\\anaconda3\\lib\\site-packages\\sklearn\\neighbors\\_classification.py:228: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Classifier  Accuracy    Recall  Specificity  Precision        F1  AUC\n",
      "0        SVM      0.52  0.333333     0.631579   0.363636  0.347826  NaN\n",
      "1        KNN      0.61  0.000000     1.000000   0.000000  0.000000  NaN\n",
      "2         LR      0.39  0.416667     0.368421   0.294118  0.344828  NaN\n",
      "3         DT      0.52  0.250000     0.684211   0.333333  0.285714  NaN\n",
      "4         RF      0.55  0.000000     0.894737   0.000000  0.000000  NaN\n",
      "5    XGBoost      0.58  0.416667     0.684211   0.454545  0.434783  NaN\n"
     ]
    }
   ],
   "source": [
    "# 只分C A\n",
    "for name in Stage:\n",
    "    for datatype in DataTypes:\n",
    "        result = pd.DataFrame()\n",
    "        data = pd.read_excel(f\"{name}_{datatype}.xlsx\")\n",
    "\n",
    "        # data = data.drop(len(data)-1) #最後一筆資料不要算的話\n",
    "        data = data.dropna()\n",
    "        \n",
    "        labelslist=[]\n",
    "        i=0\n",
    "        for subject in data['label']:\n",
    "            \n",
    "            if(subject== 0):\n",
    "                labelslist.append(0)\n",
    "                i +=1\n",
    "            else:\n",
    "                # print(1, end='')\n",
    "                labelslist.append(1)\n",
    "\n",
    "        data = data.drop(columns=['subject'])\n",
    "        data = data.drop(columns=['label']) \n",
    "        data = data.drop(columns=['name'])\n",
    "        #print(data)\n",
    "        #print(f\"{datatype}_{i}\")\n",
    "        #print(len(data),len(labelslist))\n",
    "        #print('columns ',len(data.columns))\n",
    "        # Classifier(data, labelslist, result, task, datatype)\n",
    "        #DNN_Model(name,data, labelslist, result,datatype, featuresCount=len(data.columns))\n",
    "        Classifier333(name,data, labelslist, result,datatype)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 分D ND ALL 也分C A\n",
    "for datatype in DataTypes:\n",
    "    for task in Tasks:\n",
    "        result = pd.DataFrame()\n",
    "        data = pd.read_excel(f\"{task}_{datatype}.xlsx\")\n",
    "\n",
    "        # data = data.drop(len(data)-1) #最後一筆資料不要算的話\n",
    "\n",
    "        for infornot in InfOrNot:\n",
    "            cols = [col for col in data.columns if col.startswith(infornot)]\n",
    "            DNDdata = data[cols]\n",
    "\n",
    "            DNDdata = DNDdata.dropna()\n",
    "            label = np.where(gameTime_df['label'] == 'A', 1, 0)\n",
    "            \n",
    "            classifierType = f\"{infornot}_\"+datatype\n",
    "\n",
    "            print(f\"{task}_{classifierType}\")\n",
    "            Classifier(DNDdata, label, result, task, classifierType)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EEGTypes = ['AF3', 'AF4', 'AF7', 'AF8', 'Fp1', 'Fp2']\n",
    "# 只分C A, EEG頻道個別分析\n",
    "for channel in EEGTypes:\n",
    "    for task in Tasks:\n",
    "        result = pd.DataFrame()\n",
    "        data = pd.read_excel(f\"{task}_EEG_{channel}.xlsx\")\n",
    "\n",
    "        # data = data.drop(len(data)-1) #最後一筆資料不要算的話\n",
    "        data = data.dropna()\n",
    "      \n",
    "        labelslist=[]\n",
    "        i=0\n",
    "        for subject in data['Subject']:\n",
    "            # label = gameTime_df.loc[(gameTime_df['subject'].isin(data['Subject'])) & (gameTime_df['task'] == TaskFull[task]), 'label']\n",
    "            label = gameTime_df.loc[(gameTime_df['subject']==subject) & (gameTime_df['task'] == TaskFull[task]), 'label']\n",
    "            \n",
    "            if(label.values=='C'):\n",
    "                labelslist.append(0)\n",
    "                i +=1\n",
    "            else:\n",
    "                # print(1, end='')\n",
    "                labelslist.append(1)\n",
    "        \n",
    "            # print(label.values, subject)\n",
    "  \n",
    "        # for item in labelslist:\n",
    "        #     print(item)\n",
    "\n",
    "        data = data.drop(columns=['Subject'])\n",
    "\n",
    "        print(f\"{task}_{channel}_{i}\")\n",
    "        print(len(data),len(labelslist))\n",
    "        DNN_Model(data, labelslist, result, task, channel, featuresCount=len(data.columns))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py37",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
